---
title: "Random Forest"
output: html_notebook
---

```{r}
library(tidyverse)
library(randomForest)
library(pracma)
```

```{r}
# Load clean numeric data
credit = read_csv("credit_numeric.csv")
#credit = read_csv("credit_lasso.csv") # Do random forest on LASSO selected variables 
```

```{r}
# Feature scaling to better process random forest
credit_scale = credit %>% select(-default_or_not)
credit_scale = tibble(credit$default_or_not, credit_scale)
colnames(credit_scale)[1] = "default"
credit_scale[-1] = scale(credit_scale[-1])
```

```{r}
# Split train and test data set
split = 0.8
train_observe = 1:as.integer(nrow(credit)*split)
train = credit_scale[train_observe,]
test = credit_scale[-train_observe,]
# Convert to data frame to do random forest algorithm
train = data.frame(train)
test = data.frame(test)
# Convert the dependent variable to characters and factors to perform classification
train$default = as.character(train$default)
train$default = as.factor(train$default)
test$default = as.character(test$default)
test$default = as.factor(test$default)
```

```{r}
# Reduce samples to improve processing speed
# Here we choose 1% of the whole sample
# The original data set takes computer too long to complete the model algorithm
set.seed(42)
reduced = 0.01
num_sample_train = as.integer(nrow(train)*reduced) 
num_sample_test = as.integer(nrow(test)*reduced)

train_reduced = sample_n(train, num_sample_train)
test_reduced =sample_n(test, num_sample_test)
```

```{r}
# Random forest training
# Optimization of hyperparameters was restricted due to computational limitations
# Here we simply choose mtry=sqrt(number of variables) and ntree=100
rf = randomForest(default~., data=train_reduced, mtry=12, ntree=100, importance=TRUE, na.action=na.roughfix)
```

```{r}
# Random forest prediction in sample and out of sample
prediction_is = predict(rf, train, type="class")
prediction_oos = predict(rf, test, type="class")
```

```{r}
# Model performance

# IS and OOS confusion matrix, true positive and false positive rate
conf_matrix_is = table(train$default,prediction_is)
conf_matrix_oos = table(test$default,prediction_oos)
FP_is = conf_matrix_is[1,2]/sum(conf_matrix_is[1,])
TP_is = conf_matrix_is[2,2]/sum(conf_matrix_is[2,])
FP_oos = conf_matrix_oos[1,2]/sum(conf_matrix_oos[1,])
TP_oos = conf_matrix_oos[2,2]/sum(conf_matrix_oos[2,])
# IS and OOS accuracy
accuracy_is = sum(diag(conf_matrix_is))/nrow(train)
accuracy_oos = sum(diag(conf_matrix_oos))/nrow(test)
# Print out the true and false positive rate of OOS prediction
print(c(FP_oos,TP_oos))
```

```{r}
# Importance of each variable
importance(rf)
# First 6 variables with highest Mean Decrease Accuracy
head(sort(importance(rf)[,"MeanDecreaseAccuracy"],decreasing=T))
```


```{r}
# Exhibit default probability which can be used to plot ROC Curve 

# Prediction by random forest in the form of default probability 
df = as_tibble(predict(rf,test,type="p"))
df$`0` = as.numeric(df$`0`)
df$`1` = as.numeric(df$`1`)
# We build a function showing the true and false positive rate if we categorize those with default probability greater than threshold p as defaulters
# The default random forest classification in R categorizes those with default probability greater than p=50% as defaulters
threshold_performance = function(p=0.5) {
  conf_matrix_oos_p = table(test$default,df$`1`>p)
  if (dim(conf_matrix_oos_p)[2]!=2 & colnames(conf_matrix_oos_p)[1]=="TRUE") {
    conf_matrix_oos_p= cbind(c(0,0),conf_matrix_oos_p)
  }
  if (dim(conf_matrix_oos_p)[2]!=2 & colnames(conf_matrix_oos_p)[1]=="FALSE") {
    conf_matrix_oos_p= cbind(conf_matrix_oos_p,c(0,0))
  }
  FP_p = conf_matrix_oos_p[1,2]/sum(conf_matrix_oos_p[1,])
  TP_p = conf_matrix_oos_p[2,2]/sum(conf_matrix_oos_p[2,])
  return(c(FP_p, TP_p))
}
```

```{r, message=FALSE, warning=FALSE}
# ROC curve with threshold points

# Create data frame "performance" to record true and false positive rate under different thresholds
threshold = sort(unique(df$`1`))
performance = matrix(rep(1,2),1)
for (i in 1:length(threshold)) {
  performance = rbind(performance,threshold_performance(threshold[i]))
}
performance = as_tibble(performance)
colnames(performance) = c("FP","TP")
performance = performance %>% arrange(TP)

# The 45 degree line starting from (0,0)    
slope1 = function(x){
  x
}

# ROC curve plot
ggplot(data=performance,aes(FP,TP))+
  geom_point(color=4,size=2)+
  geom_smooth(color=2,size=1)+
  geom_function(fun=slope1,color=1,size=1)+
  geom_point(aes(FP_oos,TP_oos),color=7,size=4)

# Area under curve (AUC)
AUC = trapz(performance$FP,performance$TP)
print(AUC)
```
The yellow point is the (FP,TP) under default 50% threshold used by the random forest classification in R. Other blue points are the (FP,TP)s under different thresholds. We have 89 unique values of default probability as exhibited in the data frame "df".










